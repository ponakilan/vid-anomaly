{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3ul2gBZFlpO0YwEn1wHYq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ponakilan/vid-anomaly/blob/main/RC_IPAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh6oTYaNu5Xn",
        "outputId": "f3efcd82-5b48-441b-8510-9f66e7b283a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! 7z x '/content/drive/MyDrive/Research Credit/IPAD_dataset.zip'\n",
        "\n",
        "! unzip -q '/content/drive/MyDrive/Research Credit/IPAD_dataset.zip' 'IPAD_dataset/R01'"
      ],
      "metadata": {
        "id": "UJis6ZUt5yIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from torchvision.transforms import ToTensor, Resize, Compose\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTImageProcessor, ViTModel\n",
        "\n",
        "SEQ_LEN = 10\n",
        "\n",
        "def load_image(path):\n",
        "    image = Image.open(path)\n",
        "    return image\n",
        "\n",
        "\n",
        "img_label_dataset = DatasetFolder(\n",
        "    root=\"IPAD_dataset/R01/training/frames\",\n",
        "    loader=load_image,\n",
        "    transform=Compose([\n",
        "        Resize((224, 224)),\n",
        "        ToTensor()\n",
        "    ]),\n",
        "    is_valid_file=lambda x: True\n",
        ")\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, dataset, seq_len=SEQ_LEN):\n",
        "        self.dataset = dataset\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to(self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.dataset) / self.seq_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx * self.seq_len\n",
        "        end_idx = min(start_idx + self.seq_len, len(self.dataset))\n",
        "        sequence = [self.dataset[i] for i in range(start_idx, end_idx)]\n",
        "        labels = [data[1] for data in sequence]\n",
        "        is_valid = all(label == labels[0] for label in labels)\n",
        "        if is_valid:\n",
        "            images = torch.stack([data[0] for data in sequence])\n",
        "            if len(images) < self.seq_len:\n",
        "                images = torch.cat([images, torch.zeros(self.seq_len - len(images), 3, 224, 224)])\n",
        "        else:\n",
        "            images = torch.zeros(self.seq_len, 3, 224, 224)\n",
        "\n",
        "        inputs = self.processor(\n",
        "            images=images,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        outputs = self.model(**inputs)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        encoding = last_hidden_states[:, 0, :]\n",
        "\n",
        "        return encoding, images\n",
        "\n",
        "dataset = ImgDataset(img_label_dataset)\n",
        "\n",
        "print(f\"Number of samples: {len(dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER0yHG8UvaW9",
        "outputId": "2d03f054-560f-4a25-8904-6183037be097"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class MultiScaleTemporalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, device, scales=[2, 4, 8, 10]):\n",
        "        super().__init__()\n",
        "        self.scales = scales\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(embed_dim, num_heads).to(device) for _ in scales\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        outputs = []\n",
        "\n",
        "        for i, window_size in enumerate(self.scales):\n",
        "            attn_device = next(self.attention_layers[i].parameters()).device\n",
        "            if window_size >= T:\n",
        "                q = k = v = x.transpose(0, 1).to(attn_device)\n",
        "                attn_output, _ = self.attention_layers[i](q, k, v)\n",
        "                outputs.append(attn_output.transpose(0, 1))\n",
        "            else:\n",
        "                local_outputs = []\n",
        "                for start in range(T - window_size + 1):\n",
        "                    chunk = x[:, start:start + window_size, :].to(attn_device)\n",
        "                    q = k = v = chunk.transpose(0, 1)\n",
        "                    attn_output, _ = self.attention_layers[i](q, k, v)\n",
        "                    local_outputs.append(attn_output.mean(0))\n",
        "                local_output = torch.stack(local_outputs, dim=1)\n",
        "                local_output = F.interpolate(local_output.transpose(1, 2), size=T, mode='linear').transpose(1, 2)\n",
        "                outputs.append(local_output)\n",
        "\n",
        "        final_output = torch.stack(outputs, dim=0).mean(0)\n",
        "        return final_output  # (B, T, D)\n",
        "\n",
        "class CNNFrameReconstructor(nn.Module):\n",
        "    def __init__(self, embed_dim=768, feature_dim=512, out_channels=3, img_size=224):\n",
        "        super(CNNFrameReconstructor, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.feature_dim = feature_dim\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, feature_dim * (img_size // 16) * (img_size // 16))\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(feature_dim, feature_dim // 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(feature_dim // 2, feature_dim // 4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(feature_dim // 4, feature_dim // 8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(feature_dim // 8, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, 50, embed_dim)\n",
        "        Output: (B, 50, C, H, W)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        x = x.view(B * T, D)\n",
        "        x = self.fc(x)\n",
        "        x = x.view(B * T, self.feature_dim, self.img_size // 16, self.img_size // 16)\n",
        "        x = self.decoder(x)\n",
        "        x = x.view(B, T, self.out_channels, self.img_size, self.img_size)\n",
        "        return x\n",
        "\n",
        "class FrameReconstructionModel(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(FrameReconstructionModel, self).__init__()\n",
        "        self.attn = MultiScaleTemporalAttention(\n",
        "            embed_dim=768,\n",
        "            num_heads=4,\n",
        "            device=device,\n",
        "            scales=[5, 10, 20]\n",
        "        ).to(device)\n",
        "        self.reconstructor = CNNFrameReconstructor().to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attn(x)\n",
        "        x = self.reconstructor(x)\n",
        "        return x.float()"
      ],
      "metadata": {
        "id": "TDRAndjPRcSf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.vision.learner import Learner\n",
        "from torch.utils.data import random_split\n",
        "from fastai.vision.all import *\n",
        "\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 6\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = FrameReconstructionModel(device=device).to(device)\n",
        "loss_func = MSELossFlat()\n",
        "\n",
        "train_size = int(0.85 * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "train_ds, valid_ds = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "dls = DataLoaders(train_dl, valid_dl)\n",
        "\n",
        "learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=Adam, lr=0.001)\n",
        "\n",
        "# learn.lr_find()"
      ],
      "metadata": {
        "id": "L-i6rLMBWzKR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fine_tune(EPOCHS)\n",
        "\n",
        "SAVE_PATH = f\"/content/drive/MyDrive/model_{SEQ_LEN}_{EPOCHS}_{learn.loss}.pth\"\n",
        "learn.save(SAVE_PATH)\n",
        "print(f\"Model saved to {SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "wZaqr_s2XED3",
        "outputId": "8739a155-49f1-451e-ad9b-01e6bc7a0a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.037117</td>\n",
              "      <td>0.024451</td>\n",
              "      <td>04:35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/5 00:00&lt;?]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='37' class='' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      33.33% [37/111 01:25&lt;02:50 0.0210]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "iqU0cEeufJxl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}