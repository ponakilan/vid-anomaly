{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea16903",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import VideoMAEImageProcessor, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372bb9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"ROOT_DIR\": \"data/IPAD/R01/training/frames\",\n",
    "    \"IMAGE_TENSOR_SHAPE\": (3, 224, 224),\n",
    "    \"MAE_BACKBONE\": \"OpenGVLab/VideoMAEv2-Large\",\n",
    "    \"SEQ_LEN\": 16,\n",
    "    \"BATCH_SIZE\": 12,\n",
    "    \"DATASET_SHUFFLE\": True,\n",
    "    \"EPOCHS\": 10\n",
    "}\n",
    "\n",
    "TRANSFORM = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4fa667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFrameReconstructor(torch.nn.Module):\n",
    "    def __init__(self, embed_dim=1024, feature_dim=512, out_channels=3, img_size=224):\n",
    "        super(CNNFrameReconstructor, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.feature_dim = feature_dim\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.fc = torch.nn.Linear(embed_dim, feature_dim * (img_size // 16) * (img_size // 16))\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(feature_dim, feature_dim // 2, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.ConvTranspose2d(feature_dim // 2, feature_dim // 4, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.ConvTranspose2d(feature_dim // 4, feature_dim // 8, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.ConvTranspose2d(feature_dim // 8, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, embed_dim) - Single embedding vector per batch item\n",
    "        Output: (B, C, H, W) - Single frame per batch item\n",
    "        \"\"\"\n",
    "        B, D = x.shape\n",
    "        x = self.fc(x)\n",
    "        x = x.view(B, self.feature_dim, self.img_size // 16, self.img_size // 16)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FramePredictor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FramePredictor, self).__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(configs[\"MAE_BACKBONE\"], trust_remote_code=True)\n",
    "        self.processor = VideoMAEImageProcessor.from_pretrained(configs[\"MAE_BACKBONE\"])    \n",
    "        self.video_mae = AutoModel.from_pretrained(configs[\"MAE_BACKBONE\"], config=config, trust_remote_code=True)\n",
    "\n",
    "        self.reconstructor = CNNFrameReconstructor()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        videos = [list(sequence) for sequence in x]\n",
    "        x = self.processor(videos, return_tensors=\"pt\")\n",
    "        x['pixel_values'] = x['pixel_values'].permute(0, 2, 1, 3, 4).to(device)\n",
    "        x = self.video_mae(**x)\n",
    "        x = self.reconstructor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c55b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_one_sequence(sequence_dir: str):\n",
    "    frame_files = [file for file in os.listdir(sequence_dir) if file.endswith(\"jpg\")]\n",
    "    sequence = torch.zeros(len(frame_files), *configs[\"IMAGE_TENSOR_SHAPE\"])\n",
    "    for i, frame_file in enumerate(frame_files):\n",
    "        frame_path = os.path.join(sequence_dir, frame_file)\n",
    "        image = Image.open(frame_path).convert(\"RGB\")\n",
    "        sequence[i] = TRANSFORM(image)\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def load_sequences(root_dir: str):\n",
    "    sequences = []\n",
    "    sequence_dirs = os.listdir(root_dir)\n",
    "    for sequence_dir in sequence_dirs:\n",
    "        sequence_dir_path = os.path.join(root_dir, sequence_dir)\n",
    "        sequence = load_one_sequence(sequence_dir_path)\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def visualize_sequence(sequence: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Expected input shape: (T, C, H, W)\n",
    "    \"\"\"\n",
    "    images = sequence.permute(0, 2, 3, 1)\n",
    "    grid_shape = math.ceil(math.sqrt(images.shape[0]))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(grid_shape, grid_shape, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0235240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences loaded: 34\n"
     ]
    }
   ],
   "source": [
    "sequences = load_sequences(configs[\"ROOT_DIR\"])\n",
    "print(f\"Number of sequences loaded: {len(sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80943936-24ba-46d3-ad7e-06626fcf3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, tensors, seq_len=10):\n",
    "        self.tensors = tensors\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.cumulative_lengths = [0]\n",
    "        for tensor in tensors:\n",
    "            valid_indices = max(0, tensor.shape[0] - seq_len)\n",
    "            self.cumulative_lengths.append(self.cumulative_lengths[-1] + valid_indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.cumulative_lengths[-1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_idx = 0\n",
    "        while idx >= self.cumulative_lengths[tensor_idx + 1]:\n",
    "            tensor_idx += 1\n",
    "        \n",
    "        start_frame = idx - self.cumulative_lengths[tensor_idx]\n",
    "        tensor = self.tensors[tensor_idx]\n",
    "        input_sequence = tensor[start_frame:start_frame + self.seq_len]\n",
    "        target_frame = tensor[start_frame + self.seq_len]\n",
    "        \n",
    "        return input_sequence, target_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e371737-5ecd-43a8-8316-ef361e772fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 545\n",
      "Number of validation batches: 61\n"
     ]
    }
   ],
   "source": [
    "generator = torch.Generator().manual_seed(0)\n",
    "\n",
    "dataset = SequenceDataset(\n",
    "    tensors=sequences,\n",
    "    seq_len=configs[\"SEQ_LEN\"]\n",
    ")\n",
    "\n",
    "train_ds, valid_ds = random_split(dataset, [0.9, 0.1], generator=generator)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=configs[\"BATCH_SIZE\"], shuffle=configs[\"DATASET_SHUFFLE\"])\n",
    "valid_dl = DataLoader(valid_ds, batch_size=configs[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_dl)}\")\n",
    "print(f\"Number of validation batches: {len(valid_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c48d2b-4ea3-4421-a6c8-456563754bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    with tqdm(train_dl, desc=\"Training\") as pb:\n",
    "        for i, batch in enumerate(pb):\n",
    "            sequence, next_frame = batch\n",
    "            sequence, next_frame = sequence.to(device), next_frame.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            predicted_next_frame = model(sequence)\n",
    "            loss = loss_func(predicted_next_frame, next_frame)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "            pb.set_postfix(loss=total_loss/(i+1))\n",
    "    return total_loss / len(train_dl)\n",
    "\n",
    "def validate_model(model):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    with tqdm(valid_dl, desc=\"Validation\") as pb:\n",
    "        for i, batch in enumerate(pb):\n",
    "            sequence, next_frame = batch\n",
    "            sequence, next_frame = sequence.to(device), next_frame.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted_next_frame = model(sequence)\n",
    "                \n",
    "            loss = loss_func(predicted_next_frame, next_frame)\n",
    "            total_loss += loss.item()\n",
    "            pb.set_postfix(loss=total_loss/(i+1))\n",
    "    return total_loss / len(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398ae0a-cb9e-4ef2-8766-c9a7e87631fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FramePredictor()\n",
    "for epoch in trange(configs[\"EPOCHS\"]):\n",
    "    print(f\"Using device {device}\")\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_loss = train_one_epoch(model)\n",
    "    valid_loss = validate_model(model)\n",
    "    print(f\"Epoch Train loss: {train_loss}\")\n",
    "    print(f\"Epoch Validation loss: {valid_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bba6963-4a14-426a-992c-774cdf214bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2331c86a105d4449843d194a55009b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_loss = validate_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
